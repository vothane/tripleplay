{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tripleplay.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vothane/tripleplay/blob/master/tripleplay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEViyV7q6cGE",
        "colab_type": "code",
        "outputId": "baf58314-c223-405a-8028-1baf5a308bb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.layers import Dense, Input\n",
        "from keras.layers import Conv2D, Flatten\n",
        "from keras.layers import Reshape, Conv2DTranspose\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import os.path"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qOkCPYBvZ01",
        "colab_type": "text"
      },
      "source": [
        "Run this cell only if you do not already have the image data.\n",
        "\n",
        "***Recommend that you not run this on your own machine.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJe4km5EqQw5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "948b62ba-c564-4df2-a65a-b708756a251b"
      },
      "source": [
        "!git clone https://github.com/vothane/tripleplay.git\n",
        "!mv tripleplay/imgs imgs\n",
        "!rm -rf tripleplay"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tripleplay'...\n",
            "remote: Enumerating objects: 122, done.\u001b[K\n",
            "remote: Counting objects: 100% (122/122), done.\u001b[K\n",
            "remote: Compressing objects: 100% (119/119), done.\u001b[K\n",
            "remote: Total 122 (delta 11), reused 105 (delta 3), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (122/122), 12.02 MiB | 33.63 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9t_mMwJBqb8",
        "colab_type": "code",
        "outputId": "ef6c33ce-beca-47e8-c671-acfa15cec5f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "f = lambda img_file : img_to_array(load_img('imgs/{}'.format(img_file), color_mode = \"grayscale\"))\n",
        "pitcher_map = {img_file[:-4] : f(img_file)\n",
        "              for img_file in os.listdir('imgs/')}\n",
        "print(np.shape(pitcher_map['Chris_Sale']))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(600, 600, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKnONoU6Q8qF",
        "colab_type": "code",
        "outputId": "0ea31df2-bb00-4bdb-87fe-17f87c8fe6b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "image_size = 600 # x=y\n",
        "\n",
        "# network parameters\n",
        "input_shape = (image_size, image_size, 1)\n",
        "batch_size = 32\n",
        "kernel_size = 3\n",
        "latent_dim = 16\n",
        "# encoder/decoder number of CNN layers and filters per layer\n",
        "layer_filters = [32, 64]\n",
        "\n",
        "# build the autoencoder model\n",
        "# first build the encoder model\n",
        "inputs = Input(shape=input_shape, name='encoder_input')\n",
        "x = inputs\n",
        "\n",
        "# stack of Conv2D(32)-Conv2D(64)\n",
        "for filters in layer_filters:\n",
        "    x = Conv2D(filters=filters,\n",
        "               kernel_size=kernel_size,\n",
        "               strides=2,\n",
        "               activation='relu',\n",
        "               padding='same')(x)\n",
        "\n",
        "# shape info needed to build decoder model so we don't do hand computation\n",
        "# the input to the decoder's first Conv2DTranspose will have this shape\n",
        "shape = K.int_shape(x)\n",
        "\n",
        "# generate the latent vector\n",
        "x = Flatten()(x)\n",
        "latent = Dense(latent_dim, name='latent_vector')(x)\n",
        "\n",
        "# instantiate encoder model\n",
        "encoder = Model(inputs, latent, name='encoder')\n",
        "encoder.summary()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_input (InputLayer)   (None, 600, 600, 1)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 300, 300, 32)      320       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 150, 150, 64)      18496     \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 1440000)           0         \n",
            "_________________________________________________________________\n",
            "latent_vector (Dense)        (None, 16)                23040016  \n",
            "=================================================================\n",
            "Total params: 23,058,832\n",
            "Trainable params: 23,058,832\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnFIYsVld56-",
        "colab_type": "code",
        "outputId": "d4f48b04-798c-4b99-867c-686ebfd0b1eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# build the decoder model\n",
        "latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
        "# use the shape (7, 7, 64) that was earlier saved\n",
        "x = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n",
        "# from vector to suitable shape for transposed conv\n",
        "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
        "\n",
        "# stack of Conv2DTranspose(64)-Conv2DTranspose(32)\n",
        "for filters in layer_filters[::-1]:\n",
        "    x = Conv2DTranspose(filters=filters,\n",
        "                        kernel_size=kernel_size,\n",
        "                        strides=2,\n",
        "                        activation='relu',\n",
        "                        padding='same')(x)\n",
        "\n",
        "# reconstruct the denoised input\n",
        "outputs = Conv2DTranspose(filters=1,\n",
        "                          kernel_size=kernel_size,\n",
        "                          padding='same',\n",
        "                          activation='sigmoid',\n",
        "                          name='decoder_output')(x)\n",
        "\n",
        "# instantiate decoder model\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "decoder.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "decoder_input (InputLayer)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1440000)           24480000  \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 150, 150, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_5 (Conv2DTr (None, 300, 300, 64)      36928     \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_6 (Conv2DTr (None, 600, 600, 32)      18464     \n",
            "_________________________________________________________________\n",
            "decoder_output (Conv2DTransp (None, 600, 600, 1)       289       \n",
            "=================================================================\n",
            "Total params: 24,535,681\n",
            "Trainable params: 24,535,681\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Eef1o_iKSXH",
        "colab_type": "code",
        "outputId": "d0083c01-eff9-44df-f79b-586a3e07dbf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# autoencoder = encoder + decoder\n",
        "# instantiate autoencoder model\n",
        "autoencoder = Model(inputs,\n",
        "                    decoder(encoder(inputs)),\n",
        "                    name='autoencoder')\n",
        "autoencoder.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_input (InputLayer)   (None, 600, 600, 1)       0         \n",
            "_________________________________________________________________\n",
            "encoder (Model)              (None, 16)                23058832  \n",
            "_________________________________________________________________\n",
            "decoder (Model)              (None, 600, 600, 1)       24535681  \n",
            "=================================================================\n",
            "Total params: 47,594,513\n",
            "Trainable params: 47,594,513\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW5pQW0zJjGp",
        "colab_type": "code",
        "outputId": "f3c14b6f-16a5-4bb9-ad79-e90dd5d91e83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "# Mean Square Error (MSE) loss function, Adam optimizer\n",
        "autoencoder.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "train_data = np.array([data for _, data in pitcher_map.items()])\n",
        "\n",
        "autoencoder.fit(train_data, train_data, epochs=15, batch_size=batch_size)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0617 23:43:23.202967 139768558991232 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "W0617 23:43:23.400922 139768558991232 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "96/96 [==============================] - 71s 742ms/step - loss: 1378.3313\n",
            "Epoch 2/15\n",
            "96/96 [==============================] - 63s 653ms/step - loss: 1374.9674\n",
            "Epoch 3/15\n",
            "96/96 [==============================] - 63s 656ms/step - loss: 1374.9085\n",
            "Epoch 4/15\n",
            "96/96 [==============================] - 63s 656ms/step - loss: 1374.8645\n",
            "Epoch 5/15\n",
            "96/96 [==============================] - 63s 654ms/step - loss: 1374.8015\n",
            "Epoch 6/15\n",
            "96/96 [==============================] - 63s 655ms/step - loss: 1374.7188\n",
            "Epoch 7/15\n",
            "96/96 [==============================] - 63s 657ms/step - loss: 1374.6765\n",
            "Epoch 8/15\n",
            "96/96 [==============================] - 63s 656ms/step - loss: 1374.6433\n",
            "Epoch 9/15\n",
            "96/96 [==============================] - 63s 652ms/step - loss: 1374.5881\n",
            "Epoch 10/15\n",
            "96/96 [==============================] - 63s 653ms/step - loss: 1374.5647\n",
            "Epoch 11/15\n",
            "96/96 [==============================] - 63s 651ms/step - loss: 1374.5709\n",
            "Epoch 12/15\n",
            "96/96 [==============================] - 62s 649ms/step - loss: 1374.5756\n",
            "Epoch 13/15\n",
            "96/96 [==============================] - 63s 651ms/step - loss: 1374.5788\n",
            "Epoch 14/15\n",
            "96/96 [==============================] - 63s 653ms/step - loss: 1374.5805\n",
            "Epoch 15/15\n",
            "96/96 [==============================] - 63s 652ms/step - loss: 1374.5833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1e2da77860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OEaEMLU4m0W",
        "colab_type": "text"
      },
      "source": [
        "first pass at tsne"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAbopDup4bhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "deep_features = encoder.predict(train_data, batch_size=batch_size)\n",
        "\n",
        "X_embedded = TSNE(n_components=2).fit_transform(deep_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TgVotl9EQ9C",
        "colab_type": "text"
      },
      "source": [
        "really this is basically nearest neighbors where closet points are pitchers who closely resemble each other. I need to\n",
        "graph these points and label them with pitchers names to make sense of it all. after that we can ascertain what these \n",
        "deep features may be, right now there are 16 and we can use ensemble algos to find an optimal number after words. i'm guessing something like arm slot (3/4 or overhand) and left/right handedness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2q60ouA7Xl1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1649
        },
        "outputId": "13e28b8b-7f84-421c-b43a-66fdc57701c0"
      },
      "source": [
        "X_embedded"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.1230102 , -2.8864439 ],\n",
              "       [ 3.015935  , -1.361749  ],\n",
              "       [-2.7690341 , -1.937283  ],\n",
              "       [ 2.627432  ,  1.2357011 ],\n",
              "       [-0.9062499 ,  3.567927  ],\n",
              "       [ 0.2653146 , -4.990888  ],\n",
              "       [-0.36850187, -2.3039494 ],\n",
              "       [ 2.9421334 , -5.188261  ],\n",
              "       [ 2.7214847 ,  0.45871472],\n",
              "       [-1.0072621 , -1.4214925 ],\n",
              "       [ 0.08863518,  3.9763267 ],\n",
              "       [-1.6808023 , -3.511221  ],\n",
              "       [ 1.0835378 , -4.3475184 ],\n",
              "       [-0.5858672 ,  1.7613571 ],\n",
              "       [ 1.2900186 , -0.33795804],\n",
              "       [ 3.9725788 , -3.5741136 ],\n",
              "       [-1.6186585 , -0.5814358 ],\n",
              "       [ 2.1624408 , -0.5831266 ],\n",
              "       [ 0.31183073, -2.4945543 ],\n",
              "       [ 0.40849626, -2.264806  ],\n",
              "       [ 1.4047298 ,  4.097754  ],\n",
              "       [ 0.24590746,  0.9957252 ],\n",
              "       [-0.5598806 , -4.4652705 ],\n",
              "       [-1.2933031 ,  2.5710597 ],\n",
              "       [ 0.11562246,  2.3999634 ],\n",
              "       [-0.57684064, -0.1192752 ],\n",
              "       [ 0.4941725 ,  0.01633733],\n",
              "       [-0.6489046 ,  3.812134  ],\n",
              "       [ 3.9903512 , -3.661071  ],\n",
              "       [ 1.7173737 ,  2.3169715 ],\n",
              "       [ 0.06749621, -0.9366135 ],\n",
              "       [-0.1332093 ,  3.57111   ],\n",
              "       [-0.2690041 ,  2.8911598 ],\n",
              "       [ 0.28659815, -3.8643053 ],\n",
              "       [ 0.8229449 ,  2.3341732 ],\n",
              "       [ 4.4900393 , -0.8097717 ],\n",
              "       [ 2.2542613 , -3.266915  ],\n",
              "       [ 1.2026438 , -0.5179018 ],\n",
              "       [-1.8248681 ,  1.3473203 ],\n",
              "       [-0.8524845 ,  3.3707514 ],\n",
              "       [ 0.71254915, -1.9578526 ],\n",
              "       [ 1.1786107 , -1.1052058 ],\n",
              "       [ 2.3848858 , -0.91240865],\n",
              "       [ 3.1358886 , -0.5220747 ],\n",
              "       [ 1.0702195 ,  3.9237936 ],\n",
              "       [ 0.7901366 , -3.2624028 ],\n",
              "       [ 2.1820884 , -1.5246006 ],\n",
              "       [ 3.5963428 , -2.176147  ],\n",
              "       [-0.16898064, -2.5707762 ],\n",
              "       [ 2.3686783 ,  1.8304846 ],\n",
              "       [ 1.7648469 , -1.2611183 ],\n",
              "       [ 2.165671  ,  3.1728723 ],\n",
              "       [-0.87370354, -2.801133  ],\n",
              "       [ 1.9063864 , -3.7320755 ],\n",
              "       [ 1.7690494 ,  3.1017952 ],\n",
              "       [ 2.5442672 , -2.2894058 ],\n",
              "       [-2.6158352 , -0.18195125],\n",
              "       [-2.264649  ,  2.9191732 ],\n",
              "       [ 1.2995436 ,  1.3385364 ],\n",
              "       [ 3.1572418 , -1.6862919 ],\n",
              "       [ 0.00574718,  2.441883  ],\n",
              "       [ 1.7439811 ,  1.1368953 ],\n",
              "       [ 0.73879474,  5.008434  ],\n",
              "       [ 1.0102239 ,  3.7176914 ],\n",
              "       [-0.88272405,  3.9859345 ],\n",
              "       [ 1.3098053 , -3.3334033 ],\n",
              "       [ 1.5020751 ,  2.7552178 ],\n",
              "       [-1.42221   ,  2.8861077 ],\n",
              "       [ 1.0484331 ,  1.9861563 ],\n",
              "       [ 4.479112  ,  0.34160367],\n",
              "       [-0.5375785 ,  0.28721753],\n",
              "       [-1.4491445 , -1.3988061 ],\n",
              "       [ 0.2128418 , -2.9063046 ],\n",
              "       [ 1.6567106 , -0.8910522 ],\n",
              "       [ 1.6791344 , -1.3107402 ],\n",
              "       [ 0.333985  , -1.1390991 ],\n",
              "       [ 0.3082711 ,  2.6947126 ],\n",
              "       [ 2.620087  ,  1.9671907 ],\n",
              "       [ 1.3176053 , -2.347805  ],\n",
              "       [-0.8258499 , -0.09003401],\n",
              "       [ 1.7441893 , -4.5421    ],\n",
              "       [ 3.085554  , -0.15822722],\n",
              "       [ 0.9074303 , -0.02556609],\n",
              "       [ 1.6488271 ,  0.01369434],\n",
              "       [ 4.2329063 , -1.1043123 ],\n",
              "       [ 4.303391  ,  0.7289225 ],\n",
              "       [-0.6413835 , -4.3086495 ],\n",
              "       [ 2.2375202 , -4.5361958 ],\n",
              "       [ 1.8101345 ,  4.727693  ],\n",
              "       [ 4.915902  , -1.090507  ],\n",
              "       [ 1.4714457 ,  1.26689   ],\n",
              "       [-1.3073571 , -3.7142525 ],\n",
              "       [ 2.5624213 , -4.9375153 ],\n",
              "       [ 2.471788  , -1.667304  ],\n",
              "       [-1.2085136 ,  0.93963236],\n",
              "       [-0.14009395, -2.6200676 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    }
  ]
}