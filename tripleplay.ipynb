{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tripleplay.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vothane/tripleplay/blob/master/tripleplay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEViyV7q6cGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dense, Input\n",
        "from keras.layers import Conv2D, Flatten\n",
        "from keras.layers import Reshape, Conv2DTranspose\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import os.path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9t_mMwJBqb8",
        "colab_type": "code",
        "outputId": "bc03a2d7-1060-4a8d-8c7a-1240b569f39d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "f = lambda img_file : img_to_array(load_img('imgs/{}'.format(img_file), color_mode = \"grayscale\"))\n",
        "pitcher_map = {img_file[:-4] : f(img_file)\n",
        "              for img_file in os.listdir('imgs/')}\n",
        "print(np.shape(pitcher_map['Chris_Sale']))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(600, 600, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKnONoU6Q8qF",
        "colab_type": "code",
        "outputId": "069de874-1712-478c-8f41-30a1641b4b3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "image_size = 600 # x=y\n",
        "\n",
        "# network parameters\n",
        "input_shape = (image_size, image_size, 1)\n",
        "batch_size = 32\n",
        "kernel_size = 3\n",
        "latent_dim = 16\n",
        "# encoder/decoder number of CNN layers and filters per layer\n",
        "layer_filters = [32, 64]\n",
        "\n",
        "# build the autoencoder model\n",
        "# first build the encoder model\n",
        "inputs = Input(shape=input_shape, name='encoder_input')\n",
        "x = inputs\n",
        "\n",
        "# stack of Conv2D(32)-Conv2D(64)\n",
        "for filters in layer_filters:\n",
        "    x = Conv2D(filters=filters,\n",
        "               kernel_size=kernel_size,\n",
        "               strides=2,\n",
        "               activation='relu',\n",
        "               padding='same')(x)\n",
        "\n",
        "# shape info needed to build decoder model so we don't do hand computation\n",
        "# the input to the decoder's first Conv2DTranspose will have this shape\n",
        "shape = K.int_shape(x)\n",
        "\n",
        "# generate the latent vector\n",
        "x = Flatten()(x)\n",
        "latent = Dense(latent_dim, name='latent_vector')(x)\n",
        "\n",
        "# instantiate encoder model\n",
        "encoder = Model(inputs, latent, name='encoder')\n",
        "encoder.summary()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_input (InputLayer)   (None, 600, 600, 1)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 300, 300, 32)      320       \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 150, 150, 64)      18496     \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 1440000)           0         \n",
            "_________________________________________________________________\n",
            "latent_vector (Dense)        (None, 16)                23040016  \n",
            "=================================================================\n",
            "Total params: 23,058,832\n",
            "Trainable params: 23,058,832\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnFIYsVld56-",
        "colab_type": "code",
        "outputId": "419c5a87-fd91-45d3-bb31-30bd3ad7494f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# build the decoder model\n",
        "latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
        "# use the shape (7, 7, 64) that was earlier saved\n",
        "x = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n",
        "# from vector to suitable shape for transposed conv\n",
        "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
        "\n",
        "# stack of Conv2DTranspose(64)-Conv2DTranspose(32)\n",
        "for filters in layer_filters[::-1]:\n",
        "    x = Conv2DTranspose(filters=filters,\n",
        "                        kernel_size=kernel_size,\n",
        "                        strides=2,\n",
        "                        activation='relu',\n",
        "                        padding='same')(x)\n",
        "\n",
        "# reconstruct the denoised input\n",
        "outputs = Conv2DTranspose(filters=1,\n",
        "                          kernel_size=kernel_size,\n",
        "                          padding='same',\n",
        "                          activation='sigmoid',\n",
        "                          name='decoder_output')(x)\n",
        "\n",
        "# instantiate decoder model\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "decoder.summary()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "decoder_input (InputLayer)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1440000)           24480000  \n",
            "_________________________________________________________________\n",
            "reshape_5 (Reshape)          (None, 150, 150, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_9 (Conv2DTr (None, 300, 300, 64)      36928     \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_10 (Conv2DT (None, 600, 600, 32)      18464     \n",
            "_________________________________________________________________\n",
            "decoder_output (Conv2DTransp (None, 600, 600, 1)       289       \n",
            "=================================================================\n",
            "Total params: 24,535,681\n",
            "Trainable params: 24,535,681\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Eef1o_iKSXH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "91d3f74e-6ff6-44c6-879b-589f5f3599b2"
      },
      "source": [
        "# autoencoder = encoder + decoder\n",
        "# instantiate autoencoder model\n",
        "autoencoder = Model(inputs,\n",
        "                    decoder(encoder(inputs)),\n",
        "                    name='autoencoder')\n",
        "autoencoder.summary()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_input (InputLayer)   (None, 600, 600, 1)       0         \n",
            "_________________________________________________________________\n",
            "encoder (Model)              (None, 16)                23058832  \n",
            "_________________________________________________________________\n",
            "decoder (Model)              (None, 600, 600, 1)       24535681  \n",
            "=================================================================\n",
            "Total params: 47,594,513\n",
            "Trainable params: 47,594,513\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW5pQW0zJjGp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "005f6500-f928-4d04-be93-2863b972e035"
      },
      "source": [
        "# Mean Square Error (MSE) loss function, Adam optimizer\n",
        "autoencoder.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "train_data = np.array([data for _, data in pitcher_map.items()])\n",
        "\n",
        "autoencoder.fit(train_data, train_data, epochs=1, batch_size=batch_size)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/1\n",
            "96/96 [==============================] - 103s 1s/step - loss: 1378.7513\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4e597729e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    }
  ]
}